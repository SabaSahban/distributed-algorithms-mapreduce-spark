{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8c5122",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  دیتاست اول(COVID)\n",
    " </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69974818",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "##  اتصال به کلاستر اسپارک و هدوپ \n",
    "\n",
    "    در این قسمت از تمرین باید به عنوان درایور یک سسشن  به کلاستر اسپارک بسازیم.\n",
    " </div>\n",
    "  \n",
    "* **appName**: application name displayed at the Spark Master Web UI;\n",
    "* **master**: Spark Master URL, same used by Spark Workers;\n",
    "* **spark.executor.memory**: must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1df1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.47.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading pillow-10.2.0-cp38-cp38-manylinux_2_28_aarch64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Downloading matplotlib-3.7.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (285 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.47.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp38-cp38-manylinux_2_28_aarch64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.47.0 kiwisolver-1.4.5 matplotlib-3.7.4 pillow-10.2.0 pyparsing-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4857442f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:35:08.814003Z",
     "start_time": "2024-01-09T13:35:06.691469Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e94b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 17:19:46,040 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d7bee9c29454:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-notebook-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff60bef190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('spark-notebook-1').master('spark://spark-master:7077').config('spark.executor.memmory', '2G').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d530c59",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  خواندن داده \n",
    "\n",
    "    :در اسپارک ما ساختارهای مختلفی برای کار با داده و پخش شدن آن‌ها در شبکه داریم که به ۳ دسته تقسیم بندی میشوند \n",
    "+ RDD\n",
    "+ Dataset\n",
    "+ DataFrame\n",
    "    \n",
    "    \n",
    "     برای مطالعه بیشتر به لینک زیر مراجعه کنید:\n",
    "[rdd-vs-dataframe-vs-dataset](https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset)\n",
    "\n",
    "    ما در درس با ساختار RDD آشنا شدیم حال در این تمرین میخواهیم با ساختار Dataframe آشنا شویم و به کمک آن دیتا را از روی HDFS بخوانیم و روی آن فایل‌ها پردازش انجام دهیم\n",
    " \n",
    " </div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11de0423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv ./data/covid.csv\n",
      "Loading data from csv ./data/.ipynb_checkpoints/covid-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "# The data from covid.csv will be loaded from a local file path\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"covid\" in name:\n",
    "            csvName = name\n",
    "            csvPath = os.path.join(path, name)\n",
    "            print(\"Loading data from csv {}\".format(csvPath))\n",
    "            covidDfPandas = pandas.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6d2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of data in the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"dateRep\", StringType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"cases\", IntegerType(), True),\n",
    "    StructField(\"deaths\", IntegerType(), True),\n",
    "    StructField(\"countriesAndTerritories\", StringType(), True),\n",
    "    StructField(\"geoId\", StringType(), True),\n",
    "    StructField(\"countryterritoryCode\", StringType(), True),\n",
    "    StructField(\"popData2019\", FloatType(), True),\n",
    "    StructField(\"continentExp\", StringType(), True),\n",
    "    StructField(\"Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b168e58",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "یک DataFrame از Pandas را به یک DataFrame از Spark تبدیل کنید و سپس طرح ساختاری و برخی از داده‌ها را نمایش دهید.\n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6928e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dateRep: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- countriesAndTerritories: string (nullable = true)\n",
      " |-- geoId: string (nullable = true)\n",
      " |-- countryterritoryCode: string (nullable = true)\n",
      " |-- popData2019: float (nullable = true)\n",
      " |-- continentExp: string (nullable = true)\n",
      " |-- Cumulative_number_for_14_days_of_COVID-19_cases_per_100000: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame with defined schema\n",
    "covidDfSpark = spark.createDataFrame(covidDfPandas.values.tolist(), schema=schema)\n",
    "\n",
    "# Show the DataFrame schema and some data\n",
    "covidDfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f0ce8",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک SQL \n",
    "\n",
    "    مهمترین قابلیت اسپارک این است که می‌تواند با خواندن فایل‌ها به صورت توزیع شده روی آن‌ها پردازش انجام دهد و این پردازش را برنامه ‌نویس میتواند با استفاده از دستورات SQL اعمال کند\n",
    "    در این بخش از شما انتظار می‌رود که به وسیله spark SQL  به اسپارک کوئری  بزنید . \n",
    "    \n",
    "    \n",
    " </div>\n",
    "\n",
    "[pyspark.sql.functions.col](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.count](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.groupBy](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc59c1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "برای اینکه بتوانیم روی دیتای لود شده به وسیله تابع  spark.sql\n",
    "    کوئری‌های SQL بزنیم\n",
    "    باید دو دیتاست لود شده را به عنوان table\n",
    "    به spark \n",
    "    معرفی کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5303336",
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDfSpark.registerTempTable(\"covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cebf1",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " ده رکورد آخر را نمایش دهید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5751a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 17:19:54,294 WARN scheduler.TaskSetManager: Stage 0 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (1 + 1) / 2]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1217, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m last_10_records \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM covid ORDER BY dateRep DESC LIMIT 10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlast_10_records\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1217\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_10_records = spark.sql(\"SELECT * FROM covid ORDER BY dateRep DESC LIMIT 10\")\n",
    "last_10_records.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d728b",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "اسکیما یا ساختار دیتاست ها را نمایش دهید\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ef4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "spark.sql(\"DESCRIBE EXTENDED covid;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd09712",
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7be23",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "داده‌های مربوط به ستون   countryterritoryCode ,deaths را پرینت کنید(۱۰تا) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "spark.sql(\"SELECT countryterritoryCode, deaths FROM covid LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a92946",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "تعداد کل موارد Covid برابر است با?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ecf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "spark.sql(\"SELECT SUM(cases) AS total_cases FROM covid\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc05dc",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "مجموع موارد کروناویروس در اکتبر ۲۰۲۰ در بلژیک را به دست آورید.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb278d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "covid_cases_belgium_october_2020 = spark.sql(\n",
    "    \"SELECT SUM(cases) AS total_cases_october_2020_belgium \\\n",
    "    FROM covid \\\n",
    "    WHERE month = 10 AND year = 2020 AND countriesAndTerritories = 'Belgium'\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafbf79",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "جمع تعداد موارد و مرگ‌ها در کشورهای آسیا به دست آورید.\n",
    " \n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "total_cases_deaths_by_country_asia = spark.sql(\n",
    "    \"SELECT countriesAndTerritories, SUM(cases) AS total_cases, SUM(deaths) AS total_deaths \\\n",
    "    FROM covid \\\n",
    "    WHERE continentExp = 'Asia' \\\n",
    "    GROUP BY countriesAndTerritories\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385ce93",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "میانگین تعداد موارد و مرگ‌ها در کشورهای آسیا به دست آورید.\n",
    " \n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a23789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "spark.sql(\n",
    "    \"SELECT countriesAndTerritories, AVG(cases) AS average_cases, AVG(deaths) AS average_deaths \\\n",
    "    FROM covid \\\n",
    "    WHERE continentExp = 'Asia' \\\n",
    "    GROUP BY countriesAndTerritories\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77347b62",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "جمع مرگ‌های کووید در اروپا بر اساس تاریخ به دست آورید.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "spark.sql(\n",
    "    \"SELECT dateRep, SUM(deaths) AS total_deaths \\\n",
    "    FROM covid \\\n",
    "    WHERE continentExp = 'Europe' \\\n",
    "    GROUP BY dateRep \\\n",
    "    ORDER BY dateRep\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d53f74",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک toPandas \n",
    "\n",
    "    یکی ار قابلیت‌های اسپارک این است که می‌توان dataframe های آن را به \n",
    "    dataframe های pandas تبدیل کند و از توابع آن از جمله\n",
    "    توابع plot  آن برای رسم نمودار استفاده کرد.\n",
    "    در این قسمت از شما انتظار می‌رود نمودار تعداد جنگ‌ها بر اساس هر گونه را رسم کنید.\n",
    "    \n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[pyspark.sql.DataFrame.toPandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html)\n",
    "\n",
    "[pyspark.pandas.DataFrame.plot](https://spark.apache.org/docs/3.2.1/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.plot.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2037559",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "جمع موارد و مرگ‌ها برای هر قاره را به دست آورید.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c77006",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_deaths_by_continent = spark.sql(\n",
    "    \"SELECT continentExp, SUM(cases) AS total_cases, SUM(deaths) AS total_deaths \\\n",
    "    FROM covid \\\n",
    "    GROUP BY continentExp\"\n",
    ")\n",
    "\n",
    "pandas_df = cases_deaths_by_continent.toPandas()\n",
    "\n",
    "pandas_df.plot(kind='bar', x='continentExp', y=['total_cases', 'total_deaths'], title='Total Cases and Deaths by Continent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde63828",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "نمودار مبتلایان در طور زمان را برای کشور ایران را بنویسید. \n",
    "    \n",
    " </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "covid_iran = covidDfSpark.filter(covidDfSpark.countriesAndTerritories == 'Iran')\n",
    "\n",
    "covid_iran_pandas = covid_iran.toPandas()\n",
    "\n",
    "covid_iran_pandas['dateRep'] = pd.to_datetime(covid_iran_pandas['dateRep']) \n",
    "covid_iran_pandas.sort_values(by='dateRep', inplace=True) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(covid_iran_pandas['dateRep'], covid_iran_pandas['cases'], color='purple')\n",
    "plt.title('COVID-19 Cases Over Time in Iran')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named covidDfSpark\n",
    "covid_iran.select('dateRep').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e61644",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک Shuffle \n",
    "\n",
    "    Spark SQL shuffle مکانیزمی است برای توزیع مجدد یا پارتیشن بندی مجدد داده ها به طوری که داده ها به طور متفاوت در پارتیشن ها گروه بندی می شوند، بر اساس اندازه داده شما ممکن است نیاز باشد تعداد پارتیشن های RDD/DataFrame را با استفاده از اسپارک کاهش یا افزایش دهید.\n",
    "    برای مثال وقتی روی دو dataframe مختلف که روی شبکه توزیع شده اند\n",
    "    دستور join را میزنیم یک عملیات \n",
    "    shuffling انجام میشود\n",
    "    در این قسمت از شما انتظار می‌رود کوئری جوین زیر را نوشته و اجرا کنید همچنین به  صفحه \n",
    "    application master ui\n",
    "    مراجعه کنید و نحوه shuffleing را گزارش کنید . \n",
    "     و همچنین توضیح دهید DAG scheduler  در اسپارک چیست ؟\n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[shuffling in standalone cluster](https://medium.com/@rachit1arora/apache-spark-shuffle-service-there-are-more-than-one-options-c1a8e098230e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ad6aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "از desc در PySpark برای مرتب‌سازی نزولی استفاده  کرده و با استفاده از alias یک self-join روی DataFrame covidDfSpark انجام دهید، سپس با گروه‌بندی بر اساس ستون a.continentExp از داده‌های پیوسته، تعداد وقوع‌ها شمرده شده و نتایج نمایش دهید. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "covidDfSpark.alias(\"a\").join(covidDfSpark.alias(\"b\"), \"continentExp\") \\\n",
    "    .groupBy(\"a.continentExp\") \\\n",
    "    .agg(F.sum(\"a.cases\").alias(\"total_cases\")) \\\n",
    "    .orderBy(F.desc(\"continentExp\")) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochNow = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb6f15",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "نوشتن یکDataFrame به عنوان یک فایل parquet در HDFS\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDfSpark.write.parquet(\"hdfs://namenode:8020/covid/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Covid Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f27853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from HDFS to confirm it was successfully stored\n",
    "\n",
    "df_load = spark.read.parquet(\"hdfs://namenode:8020/covid/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Covid Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d675a4",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  دیتاست دوم(پترن‌های باکتری Ecoli)\n",
    " </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new SparkSession\n",
    "spark2 = SparkSession.builder.appName('spark-notebook-2').master('spark://spark-master:7077').getOrCreate()\n",
    "\n",
    "# Create the schema\n",
    "schema = StructType([\n",
    "    StructField(\"0\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data from covid.csv will be loaded from a local file path\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"ecoli\" in name and name.endswith(\".txt\"):\n",
    "            txtPath = os.path.join(path, name)\n",
    "            print(\"Loading data from text file: {}\".format(txtPath))\n",
    "            # Load the ecoli.txt file\n",
    "            lines_txt = pandas.read_csv(txtPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new SparkSession\n",
    "spark2 = SparkSession.builder.appName('spark-notebook-2').master('spark://spark-master:7077').getOrCreate()\n",
    "\n",
    "# Create the schema\n",
    "schema = StructType([StructField(\"0\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_sp = spark2.createDataFrame(lines_txt, schema=schema)\n",
    "# Preview the structure\n",
    "lines_sp.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a96d3",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک RDD \n",
    "\n",
    "    \n",
    " </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cba8d7",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "DataFrame را به RDD تبدیل کنید.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do\n",
    "lines_rdd = lines_sp.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lengths for patterns (3 and 4)\n",
    "pattern_lengths = [3, 4]\n",
    "lines_rdd = lines_rdd.map(lambda row: row[0])\n",
    "lines_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_grams(line, length):\n",
    "    ngrams = []\n",
    "    for i in range(len(line) - length + 1):\n",
    "        ngrams.append(line[i : i + length])\n",
    "    return ngrams\n",
    "\n",
    "pattern_lengths = [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef9d0b",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "ایجاد RDD برای هر طول الگو (n-gram):\n",
    " \n",
    "ابتدا یک لیست از RDDهای n-gramها برای هر طول الگو (pattern length) ایجاد کرده.\n",
    "برای هر طول الگو، با استفاده از flatMap بر روی هر خط موجود در مجموعه داده (lines_rdd)، n-gramهای مربوط به طول الگو مورد نظر را ایجاد کرده و به لیستی از RDDها اضافه کنید.\n",
    " </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d32226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_rdds = []\n",
    "for length in pattern_lengths:\n",
    "    #to-do\n",
    "    ngram_rdd = lines_rdd.flatMap(lambda line: line_to_grams(line, length))\n",
    "    ngrams_rdds.append(ngram_rdd)    \n",
    "ngrams_rdds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1bddf",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    " محاسبه تعداد n-gramها:\n",
    "سپس، برای هر RDD حاصل از مرحله قبل، تعداد تکرارهای هر n-gram را محاسبه کرده.\n",
    "این کار با استفاده از map و reduceByKey بر روی هر n-gram، جفت‌هایی از (n-gram, تعداد تکرار) را ایجاد کرده و سپس نتایج را به یک لیست از RDDها اضافه کرده.\n",
    "در نهایت، دو RDD ایجاد شده داده‌های مربوط به هر طول الگو را در اختیار دارند.\n",
    "     </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b180719",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rdds = []\n",
    "\n",
    "def count_ngrams(rdd):\n",
    "    return rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Calculate counts of n-grams for each RDD and pattern length\n",
    "for ngrams_rdd, length in zip(ngrams_rdds, pattern_lengths):\n",
    "    # Calculate counts for each n-gram in the RDD using map and reduceByKey\n",
    "    ngram_counts = count_ngrams(ngrams_rdd)\n",
    "    # Append the RDD containing n-gram counts to the list\n",
    "    count_rdds.append(ngram_counts)\n",
    "count_rdds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rdd = count_rdds[0].union(count_rdds[1])\n",
    "dna_patterns = merged_rdd.sortByKey(ascending=False).map(lambda x: f\"{x[1]} {x[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_patterns.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each string into two columns (ngram and count) using space as the delimiter\n",
    "split_rdd = dna_patterns.map(lambda line: line.split())\n",
    "\n",
    "# Create a DataFrame from the RDD with column names\n",
    "columns = [\"pattern\", \"count\"]\n",
    "df = spark2.createDataFrame(split_rdd, columns)\n",
    "split_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2ca9f",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "# تابع تعریف شده توسط کاربر(UDF)\n",
    "\n",
    "یکی از مزایای اسپارک این است که نه تنها برای ما یک زبان SQL فراهم کرده که روی چندین سرور به صورت همزمان پردازش را انجام دهد بلکه \n",
    "    میتوان به زبان‌های مختلف توابعی تعریف کرد که روی  همه executor ها اجرا شود   در این بخش از شما انتظار می‌رود که با نوشتن یک UDF به زبان پایتونی    توضیحات  قسمت  قبل را انجام دهید.\n",
    "\n",
    " </div>\n",
    " \n",
    "[pyspark.sql.functions.udf](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.withColumn](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(line):\n",
    "    parts = line.split()  # Splitting assuming space as delimiter\n",
    "    return parts[0], int(parts[1]) if len(parts) > 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ca9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define the UDF to split the string into two columns\n",
    "# to-do\n",
    "returnType = StructType([StructField(\"ngram\", StringType(), True),StructField(\"count\", IntegerType(), True)])\n",
    "split_string_udf = udf(split_string, returnType)\n",
    "\n",
    "# Apply the UDF to create separate columns\n",
    "df_with_split = df.withColumn(\"split\", split_string_udf(df[\"count\"])).select(\"split.ngram\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# to-do\n",
    "df_with_split.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
